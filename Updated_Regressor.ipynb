{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd7dbf8-dd64-4b12-aba5-ece09ad19189",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ff1f9-e211-4728-ad84-72bae7ae4ab7",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f1177b-f2b8-4b48-a52f-ab28fdcd9082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openpyxl #install openpyxl for opening .xlsx data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db331b01-07c1-4e1f-a739-c7279302c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eef880-9c01-41b3-b93b-947b03ab8b72",
   "metadata": {},
   "source": [
    "### Step 2: Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411d037a-a4f7-454e-ad7c-d93ec5cd049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preview of data from Year 2009-2010:\n",
      "  Invoice StockCode                          Description  Quantity  \\\n",
      "0  489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
      "1  489434    79323P                   PINK CHERRY LIGHTS        12   \n",
      "2  489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
      "3  489434     22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
      "4  489434     21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
      "\n",
      "          InvoiceDate  Price  Customer ID         Country  \n",
      "0 2009-12-01 07:45:00   6.95      13085.0  United Kingdom  \n",
      "1 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "2 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "3 2009-12-01 07:45:00   2.10      13085.0  United Kingdom  \n",
      "4 2009-12-01 07:45:00   1.25      13085.0  United Kingdom  \n",
      "\n",
      "Preview of data from Year 2010-2011:\n",
      "  Invoice StockCode                          Description  Quantity  \\\n",
      "0  536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1  536365     71053                  WHITE METAL LANTERN         6   \n",
      "2  536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
      "3  536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
      "4  536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
      "\n",
      "          InvoiceDate  Price  Customer ID         Country  \n",
      "0 2010-12-01 08:26:00   2.55      17850.0  United Kingdom  \n",
      "1 2010-12-01 08:26:00   3.39      17850.0  United Kingdom  \n",
      "2 2010-12-01 08:26:00   2.75      17850.0  United Kingdom  \n",
      "3 2010-12-01 08:26:00   3.39      17850.0  United Kingdom  \n",
      "4 2010-12-01 08:26:00   3.39      17850.0  United Kingdom  \n",
      "Combining data...\n",
      "\n",
      "Preview of combined data:\n",
      "  Invoice StockCode                          Description  Quantity  \\\n",
      "0  489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
      "1  489434    79323P                   PINK CHERRY LIGHTS        12   \n",
      "2  489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
      "3  489434     22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
      "4  489434     21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
      "\n",
      "          InvoiceDate  Price  Customer ID         Country  \n",
      "0 2009-12-01 07:45:00   6.95      13085.0  United Kingdom  \n",
      "1 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "2 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "3 2009-12-01 07:45:00   2.10      13085.0  United Kingdom  \n",
      "4 2009-12-01 07:45:00   1.25      13085.0  United Kingdom  \n",
      "Dropping missing values...\n",
      "\n",
      "Data after dropping missing values:\n",
      "  Invoice StockCode                          Description  Quantity  \\\n",
      "0  489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
      "1  489434    79323P                   PINK CHERRY LIGHTS        12   \n",
      "2  489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
      "3  489434     22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
      "4  489434     21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
      "\n",
      "          InvoiceDate  Price  Customer ID         Country  \n",
      "0 2009-12-01 07:45:00   6.95      13085.0  United Kingdom  \n",
      "1 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "2 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "3 2009-12-01 07:45:00   2.10      13085.0  United Kingdom  \n",
      "4 2009-12-01 07:45:00   1.25      13085.0  United Kingdom  \n",
      "Encoding categorical variables...\n",
      "\n",
      "Data after encoding categorical variables:\n",
      "  Invoice StockCode                          Description  Quantity  \\\n",
      "0  489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
      "1  489434    79323P                   PINK CHERRY LIGHTS        12   \n",
      "2  489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
      "3  489434     22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
      "4  489434     21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
      "\n",
      "          InvoiceDate  Price  Customer ID  Country  \n",
      "0 2009-12-01 07:45:00   6.95      13085.0       38  \n",
      "1 2009-12-01 07:45:00   6.75      13085.0       38  \n",
      "2 2009-12-01 07:45:00   6.75      13085.0       38  \n",
      "3 2009-12-01 07:45:00   2.10      13085.0       38  \n",
      "4 2009-12-01 07:45:00   1.25      13085.0       38  \n",
      "\n",
      "Data with TotalValue column added:\n",
      "  Invoice StockCode                          Description  Quantity  \\\n",
      "0  489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
      "1  489434    79323P                   PINK CHERRY LIGHTS        12   \n",
      "2  489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
      "3  489434     22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
      "4  489434     21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
      "\n",
      "          InvoiceDate  Price  Customer ID  Country  TotalValue  \n",
      "0 2009-12-01 07:45:00   6.95      13085.0       38        83.4  \n",
      "1 2009-12-01 07:45:00   6.75      13085.0       38        81.0  \n",
      "2 2009-12-01 07:45:00   6.75      13085.0       38        81.0  \n",
      "3 2009-12-01 07:45:00   2.10      13085.0       38       100.8  \n",
      "4 2009-12-01 07:45:00   1.25      13085.0       38        30.0  \n",
      "Sampling subset of data...\n",
      "\n",
      "Preview of the sampled subset:\n",
      "       Invoice StockCode                          Description  Quantity  \\\n",
      "723499  554000     23281       FOLDING BUTTERFLY MIRROR RED          12   \n",
      "982498  575729     22406        MONEY BOX KINGS CHOICE DESIGN        12   \n",
      "236404  512286     22435       SET OF 9 HEART SHAPED BALLOONS        10   \n",
      "435239  530957     22800  ANTIQUE TALL SWIRLGLASS TRINKET POT         8   \n",
      "740008  555586     84212        ASSORTED FLOWER COLOUR \"LEIS\"        72   \n",
      "\n",
      "               InvoiceDate  Price  Customer ID  Country  TotalValue  \n",
      "723499 2011-05-20 11:53:00   0.83      14794.0       38        9.96  \n",
      "982498 2011-11-10 19:49:00   1.25      17932.0       38       15.00  \n",
      "236404 2010-06-14 14:27:00   1.25      15967.0       38       12.50  \n",
      "435239 2010-11-04 18:53:00   3.75      17340.0       38       30.00  \n",
      "740008 2011-06-06 09:12:00   0.65      14232.0       38       46.80  \n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "# Read all sheets into a dictionary\n",
    "sheets = pd.read_excel(\"online_retail_II.xlsx\", sheet_name=None)\n",
    "\n",
    "# Extract individual sheets\n",
    "data_2009_2010 = sheets['Year 2009-2010']\n",
    "data_2010_2011 = sheets['Year 2010-2011']\n",
    "\n",
    "# Display the first few rows of each sheet\n",
    "print(\"Preview of data from Year 2009-2010:\")\n",
    "print(data_2009_2010.head())\n",
    "print(\"\\nPreview of data from Year 2010-2011:\")\n",
    "print(data_2010_2011.head())\n",
    "\n",
    "# Combine both sheets for analysis\n",
    "print(\"Combining data...\")\n",
    "data = pd.concat([data_2009_2010, data_2010_2011], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined data\n",
    "print(\"\\nPreview of combined data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Drop rows with missing values\n",
    "print(\"Dropping missing values...\")\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Display the data after dropping missing values\n",
    "print(\"\\nData after dropping missing values:\")\n",
    "print(data.head())\n",
    "\n",
    "# Encode categorical variables\n",
    "print(\"Encoding categorical variables...\")\n",
    "le_country = LabelEncoder()\n",
    "data['Country'] = le_country.fit_transform(data['Country'])\n",
    "\n",
    "# Display data after encoding\n",
    "print(\"\\nData after encoding categorical variables:\")\n",
    "print(data.head())\n",
    "\n",
    "# Create a target variable (Total Invoice Value)\n",
    "data['TotalValue'] = data['Quantity'] * data['Price']\n",
    "\n",
    "# Display data with the new target variable\n",
    "print(\"\\nData with TotalValue column added:\")\n",
    "print(data.head())\n",
    "\n",
    "# Select a subset for faster processing\n",
    "print(\"Sampling subset of data...\")\n",
    "data_subset = data.sample(n=5000, random_state=42)\n",
    "\n",
    "# Display the sampled subset\n",
    "print(\"\\nPreview of the sampled subset:\")\n",
    "print(data_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9bfffb-c354-4ede-b2d2-46d114495984",
   "metadata": {},
   "source": [
    "### Step 3: Define Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5194345c-7e27-4f52-8564-3c263e6a3e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of Features (X):\n",
      "        Quantity  Price  Country\n",
      "723499        12   0.83       38\n",
      "982498        12   1.25       38\n",
      "236404        10   1.25       38\n",
      "435239         8   3.75       38\n",
      "740008        72   0.65       38\n",
      "\n",
      "Preview of Target (y):\n",
      "723499     9.96\n",
      "982498    15.00\n",
      "236404    12.50\n",
      "435239    30.00\n",
      "740008    46.80\n",
      "Name: TotalValue, dtype: float64\n",
      "Scaling features...\n",
      "\n",
      "Preview of Scaled Features:\n",
      "   Quantity     Price   Country\n",
      "0 -0.013501 -0.105337  0.315146\n",
      "1 -0.013501 -0.087179  0.315146\n",
      "2 -0.032863 -0.087179  0.315146\n",
      "3 -0.052224  0.020905  0.315146\n",
      "4  0.567352 -0.113119  0.315146\n",
      "Splitting data into training and testing sets...\n",
      "\n",
      "Shapes of training and testing sets:\n",
      "X_train: (3500, 3), X_test: (1500, 3)\n",
      "y_train: (3500,), y_test: (1500,)\n"
     ]
    }
   ],
   "source": [
    "# Define the independent variables (features) and the dependent variable (target)\n",
    "X = data_subset[['Quantity', 'Price', 'Country']]\n",
    "y = data_subset['TotalValue']\n",
    "\n",
    "# Display the first few rows of features and target\n",
    "print(\"Preview of Features (X):\")\n",
    "print(X.head())\n",
    "print(\"\\nPreview of Target (y):\")\n",
    "print(y.head())\n",
    "\n",
    "# Scale features\n",
    "print(\"Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Display the first few rows of scaled features\n",
    "print(\"\\nPreview of Scaled Features:\")\n",
    "print(pd.DataFrame(X_scaled, columns=['Quantity', 'Price', 'Country']).head())\n",
    "\n",
    "# Split data into training and testing sets\n",
    "print(\"Splitting data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Display the shape of the training and testing sets\n",
    "print(\"\\nShapes of training and testing sets:\")\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}, y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6d27d7-6e87-49ae-a0ee-ea973c294ea5",
   "metadata": {},
   "source": [
    "The dataset was prepared for modeling by defining the independent variables (Quantity, Price, and Country) as features and TotalValue as the target variable. The features were scaled using StandardScaler to normalize the data, ensuring that all variables are on a comparable scale. After scaling, the data was split into training and testing sets, with 70% allocated for training the model and 30% reserved for testing. The training set contains 3,500 samples, while the testing set includes 1,500 samples. This process ensures the model is trained on a balanced dataset and evaluated on unseen data to assess its performance accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa7a6f2-52a3-4bda-9804-d3fad22afcb8",
   "metadata": {},
   "source": [
    "# Step 4: Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93911d1-e596-43bc-af5a-4c92d9dac059",
   "metadata": {},
   "source": [
    "## 4.0 Checking all regression model MSE without tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5095620f-468e-40ad-9b76-a45f87bfe030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison based on MSE:\n",
      "Linear Regression: MSE = 3291.06\n",
      "Ridge Regression: MSE = 3291.06\n",
      "Lasso Regression: MSE = 3290.64\n",
      "ElasticNet Regression: MSE = 3290.67\n",
      "Decision Tree Regressor: MSE = 1123.52\n",
      "Random Forest Regressor: MSE = 650.09\n",
      "Gradient Boosting Regressor: MSE = 550.25\n",
      "SVR: MSE = 4245.32\n",
      "KNN Regressor: MSE = 1538.91\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets (replace X, y with your data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models to be evaluated\n",
    "models = {\n",
    "    'Linear Regression': Ridge(),\n",
    "    'Ridge Regression': Ridge(),\n",
    "    'Lasso Regression': Lasso(),\n",
    "    'ElasticNet Regression': ElasticNet(),\n",
    "    'Decision Tree Regressor': DecisionTreeRegressor(),\n",
    "    'Random Forest Regressor': RandomForestRegressor(),\n",
    "    'Gradient Boosting Regressor': GradientBoostingRegressor(),\n",
    "    'SVR': SVR(),\n",
    "    'KNN Regressor': KNeighborsRegressor()\n",
    "}\n",
    "\n",
    "# Store results\n",
    "model_results = {}\n",
    "\n",
    "# Loop through each model, fit it, and calculate MSE\n",
    "for model_name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # Store the result\n",
    "    model_results[model_name] = mse\n",
    "\n",
    "# Display results\n",
    "print(\"Model Comparison based on MSE:\")\n",
    "for model_name, mse in model_results.items():\n",
    "    print(f\"{model_name}: MSE = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1476f0-7362-4c83-95ab-37da82302cea",
   "metadata": {},
   "source": [
    " ## 4.1 Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9be8ab1-6e5b-44d9-af14-de00898bbd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest Regressor...\n",
      "Defining hyperparameter grid...\n",
      "Starting Grid Search...\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "\n",
      "Best Parameters Found:\n",
      "{'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "\n",
      "Training the model with the best parameters...\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "Model Evaluation:\n",
      "Mean Squared Error (MSE): 391.42\n",
      "\n",
      "Grid Search Results:\n",
      "Mean MSE: 1073.64, Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Mean MSE: 1084.84, Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Mean MSE: 1091.45, Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "Mean MSE: 1174.28, Parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Mean MSE: 1191.90, Parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Mean MSE: 1199.71, Parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 150}\n",
      "Mean MSE: 1272.21, Parameters: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Mean MSE: 1306.88, Parameters: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Mean MSE: 1312.80, Parameters: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 150}\n",
      "Mean MSE: 1087.60, Parameters: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Mean MSE: 1095.71, Parameters: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Mean MSE: 1101.91, Parameters: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "Mean MSE: 1185.14, Parameters: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Mean MSE: 1196.89, Parameters: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Mean MSE: 1203.06, Parameters: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 150}\n",
      "Mean MSE: 1275.51, Parameters: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Mean MSE: 1310.30, Parameters: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Mean MSE: 1316.28, Parameters: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 150}\n",
      "Mean MSE: 1075.09, Parameters: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Mean MSE: 1088.99, Parameters: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Mean MSE: 1094.29, Parameters: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "Mean MSE: 1174.30, Parameters: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Mean MSE: 1191.92, Parameters: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Mean MSE: 1199.82, Parameters: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 150}\n",
      "Mean MSE: 1272.21, Parameters: {'max_depth': 20, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Mean MSE: 1306.88, Parameters: {'max_depth': 20, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Mean MSE: 1312.80, Parameters: {'max_depth': 20, 'min_samples_split': 10, 'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "# Train the Random Forest Regression model and evaluate its performance\n",
    "print(\"Training Random Forest Regressor...\")\n",
    "\n",
    "# Initialize the base Random Forest model\n",
    "reg = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "print(\"Defining hyperparameter grid...\")\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # Number of trees in the forest. Lower values train faster; higher values may improve accuracy.\n",
    "    'max_depth': [None, 10, 20],     # Maximum depth of trees. None allows fully expanded trees.\n",
    "    'min_samples_split': [2, 5, 10]  # Minimum samples required to split an internal node. Higher values reduce overfitting.\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "print(\"Starting Grid Search...\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=reg, \n",
    "    param_grid=param_grid, \n",
    "    cv=3, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)  # Train the model with Grid Search\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "print(best_params)\n",
    "\n",
    "# Train the model using the best hyperparameters\n",
    "print(\"\\nTraining the model with the best parameters...\")\n",
    "best_reg = RandomForestRegressor(**best_params, random_state=42)\n",
    "best_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "print(\"\\nMaking predictions...\")\n",
    "predictions = best_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "\n",
    "# Display a summary of Grid Search results\n",
    "print(\"\\nGrid Search Results:\")\n",
    "# Loop through the Grid Search results to display each parameter combination with its mean score\n",
    "for mean_score, params in zip(grid_search.cv_results_['mean_test_score'], grid_search.cv_results_['params']):\n",
    "    print(f\"Mean MSE: {-mean_score:.2f}, Parameters: {params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942688ba-7415-4447-aac0-158258ef894f",
   "metadata": {},
   "source": [
    "The Random Forest Regressor was trained using GridSearchCV to optimize hyperparameters by systematically evaluating different combinations. A total of 27 combinations were tested across 3-fold cross-validation, resulting in 81 fits. The best parameters identified were:\n",
    "\n",
    "max_depth: None (no limit on tree depth).\n",
    "min_samples_split: 2 (minimum samples required to split an internal node).\n",
    "n_estimators: 50 (number of trees in the forest).\n",
    "This combination achieved the lowest Mean Squared Error (MSE) of 363.01 on the validation data.\n",
    "\n",
    "The grid search results display the mean MSE for each tested parameter combination. The process helps ensure that the model is tuned to balance training accuracy and generalization to unseen data. Using fewer trees (n_estimators=50) provides faster training while maintaining accuracy, and limiting min_samples_split helps the model capture finer details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63a98cf-4b8c-4f40-a91c-95c00dec4958",
   "metadata": {},
   "source": [
    "## 4.2 Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b8c556c-4a27-4a8b-825a-130d1f8212e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Regressor...\n",
      "Defining hyperparameter grid...\n",
      "Starting Grid Search...\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "\n",
      "Best Parameters Found:\n",
      "{'max_depth': 10, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\n",
      "Training the model with the best parameters...\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "Model Evaluation:\n",
      "Mean Squared Error (MSE): 951.44\n",
      "\n",
      "Grid Search Results:\n",
      "Mean MSE: 1095.33, Parameters: {'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Mean MSE: 1319.43, Parameters: {'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "Mean MSE: 1293.23, Parameters: {'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Mean MSE: 1405.52, Parameters: {'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Mean MSE: 1461.65, Parameters: {'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "Mean MSE: 1534.49, Parameters: {'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Mean MSE: 1471.92, Parameters: {'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Mean MSE: 1471.92, Parameters: {'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "Mean MSE: 1438.41, Parameters: {'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Mean MSE: 1106.39, Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Mean MSE: 1557.91, Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "Mean MSE: 1332.98, Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Mean MSE: 2200.02, Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Mean MSE: 1819.25, Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "Mean MSE: 1744.29, Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Mean MSE: 2039.61, Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Mean MSE: 2039.61, Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "Mean MSE: 2175.70, Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Mean MSE: 1106.39, Parameters: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Mean MSE: 1557.91, Parameters: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "Mean MSE: 1332.98, Parameters: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Mean MSE: 2200.02, Parameters: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Mean MSE: 1819.25, Parameters: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "Mean MSE: 1744.29, Parameters: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Mean MSE: 2039.61, Parameters: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Mean MSE: 2039.61, Parameters: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "Mean MSE: 2175.70, Parameters: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Mean MSE: 1026.61, Parameters: {'max_depth': 10, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Mean MSE: 1312.90, Parameters: {'max_depth': 10, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "Mean MSE: 1316.90, Parameters: {'max_depth': 10, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Mean MSE: 1421.00, Parameters: {'max_depth': 10, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Mean MSE: 1477.82, Parameters: {'max_depth': 10, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "Mean MSE: 1549.99, Parameters: {'max_depth': 10, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Mean MSE: 1481.58, Parameters: {'max_depth': 10, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Mean MSE: 1481.58, Parameters: {'max_depth': 10, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "Mean MSE: 1447.95, Parameters: {'max_depth': 10, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Mean MSE: 1473.23, Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Mean MSE: 1314.46, Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "Mean MSE: 1453.13, Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Mean MSE: 1660.58, Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Mean MSE: 2263.87, Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "Mean MSE: 2143.08, Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Mean MSE: 1797.95, Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Mean MSE: 1797.95, Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "Mean MSE: 1941.80, Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Mean MSE: 1473.23, Parameters: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Mean MSE: 1314.46, Parameters: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "Mean MSE: 1453.13, Parameters: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Mean MSE: 1660.58, Parameters: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Mean MSE: 2263.87, Parameters: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "Mean MSE: 2143.08, Parameters: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Mean MSE: 1797.95, Parameters: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Mean MSE: 1797.95, Parameters: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "Mean MSE: 1941.80, Parameters: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Mean MSE: 1095.33, Parameters: {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Mean MSE: 1319.43, Parameters: {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "Mean MSE: 1293.23, Parameters: {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Mean MSE: 1405.52, Parameters: {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Mean MSE: 1461.65, Parameters: {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "Mean MSE: 1534.49, Parameters: {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Mean MSE: 1471.92, Parameters: {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Mean MSE: 1471.92, Parameters: {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "Mean MSE: 1438.41, Parameters: {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Mean MSE: 1152.44, Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Mean MSE: 1708.86, Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "Mean MSE: 1483.81, Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Mean MSE: 2200.02, Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Mean MSE: 1734.70, Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "Mean MSE: 1758.61, Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Mean MSE: 2039.61, Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Mean MSE: 2039.61, Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "Mean MSE: 2175.70, Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Mean MSE: 1152.44, Parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Mean MSE: 1708.86, Parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "Mean MSE: 1483.81, Parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Mean MSE: 2200.02, Parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Mean MSE: 1734.70, Parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "Mean MSE: 1758.61, Parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Mean MSE: 2039.61, Parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Mean MSE: 2039.61, Parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "Mean MSE: 2175.70, Parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "# Train the Decision Tree Regression model and evaluate its performance\n",
    "print(\"Training Decision Tree Regressor...\")\n",
    "\n",
    "# Initialize the base Decision Tree model\n",
    "reg = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "print(\"Defining hyperparameter grid...\")\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20],  # Maximum depth of the tree. None means no limit on the depth.\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node.\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum samples required to be at a leaf node.\n",
    "    'max_features': [None, 'sqrt', 'log2']  # The number of features to consider when looking for the best split.\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "print(\"Starting Grid Search...\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=reg, \n",
    "    param_grid=param_grid, \n",
    "    cv=3, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)  # Train the model with Grid Search\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "print(best_params)\n",
    "\n",
    "# Train the model using the best hyperparameters\n",
    "print(\"\\nTraining the model with the best parameters...\")\n",
    "best_reg = DecisionTreeRegressor(**best_params, random_state=42)\n",
    "best_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "print(\"\\nMaking predictions...\")\n",
    "predictions = best_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "\n",
    "# Display a summary of Grid Search results\n",
    "print(\"\\nGrid Search Results:\")\n",
    "# Loop through the Grid Search results to display each parameter combination with its mean score\n",
    "for mean_score, params in zip(grid_search.cv_results_['mean_test_score'], grid_search.cv_results_['params']):\n",
    "    print(f\"Mean MSE: {-mean_score:.2f}, Parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d2047-13e1-42d3-9202-fed54862a1a6",
   "metadata": {},
   "source": [
    "## 4.3 Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57a93e6a-0878-461c-abd3-528c5c2ef7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Ridge Regression Model...\n",
      "Defining hyperparameter grid...\n",
      "Starting Grid Search...\n",
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n",
      "\n",
      "Best Parameters Found:\n",
      "{'alpha': 500}\n",
      "\n",
      "Training the model with the best parameters...\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "Model Evaluation:\n",
      "Mean Squared Error (MSE): 2479.01\n",
      "\n",
      "Grid Search Results:\n",
      "Mean MSE: 15480.44, Parameters: {'alpha': 0.1}\n",
      "Mean MSE: 15325.01, Parameters: {'alpha': 1}\n",
      "Mean MSE: 13906.97, Parameters: {'alpha': 10}\n",
      "Mean MSE: 7001.89, Parameters: {'alpha': 100}\n",
      "Mean MSE: 4711.65, Parameters: {'alpha': 200}\n",
      "Mean MSE: 4183.90, Parameters: {'alpha': 250}\n",
      "Mean MSE: 3838.96, Parameters: {'alpha': 300}\n",
      "Mean MSE: 3605.56, Parameters: {'alpha': 350}\n",
      "Mean MSE: 3443.45, Parameters: {'alpha': 400}\n",
      "Mean MSE: 3328.66, Parameters: {'alpha': 450}\n",
      "Mean MSE: 3246.25, Parameters: {'alpha': 500}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Train the Ridge Regression model and evaluate its performance\n",
    "print(\"Training Ridge Regression Model...\")\n",
    "\n",
    "# Initialize the base Ridge Regression model\n",
    "reg = Ridge()\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "print(\"Defining hyperparameter grid...\")\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 1, 10, 100, 200, 250, 300, 350, 400, 450, 500]  # Regularization strength for Ridge regression\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "print(\"Starting Grid Search...\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=reg, \n",
    "    param_grid=param_grid, \n",
    "    cv=3, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)  # Train the model with Grid Search\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "print(best_params)\n",
    "\n",
    "# Train the model using the best hyperparameters\n",
    "print(\"\\nTraining the model with the best parameters...\")\n",
    "best_reg = Ridge(**best_params)  # Using Ridge with the best found alpha\n",
    "best_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "print(\"\\nMaking predictions...\")\n",
    "predictions = best_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "\n",
    "# Display a summary of Grid Search results\n",
    "print(\"\\nGrid Search Results:\")\n",
    "# Loop through the Grid Search results to display each parameter combination with its mean score\n",
    "for mean_score, params in zip(grid_search.cv_results_['mean_test_score'], grid_search.cv_results_['params']):\n",
    "    print(f\"Mean MSE: {-mean_score:.2f}, Parameters: {params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548739a-444c-47eb-b9a6-5c49b1e14f80",
   "metadata": {},
   "source": [
    "## 4.4 Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb1acb99-1b3f-48ed-af3f-df72a833902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gradient Boosting Regressor...\n",
      "Defining hyperparameter grid...\n",
      "Starting Grid Search...\n",
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n",
      "\n",
      "Best Parameters Found:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.9}\n",
      "\n",
      "Training the model with the best parameters...\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "Model Evaluation:\n",
      "Mean Squared Error (MSE): 532.92\n",
      "\n",
      "Grid Search Results:\n",
      "Mean MSE: 2351.77, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 2362.27, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 2384.90, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 1811.59, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 1822.06, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 1813.70, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 1467.52, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 1460.01, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 1452.20, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 2320.09, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 2337.77, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 2351.05, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 1763.52, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 1784.63, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 1764.08, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 1422.17, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 1443.27, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 1414.54, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 2324.90, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 2357.17, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 2373.45, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 1782.43, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 1812.95, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 1792.28, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 1457.33, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 1470.21, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 1451.83, Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 2035.11, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 2028.59, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 2048.85, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 1456.36, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 1461.65, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 1464.13, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 1131.36, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 1140.28, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 1150.73, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 2015.14, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 2003.88, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 2014.10, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 1431.37, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 1431.41, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 1412.13, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 1110.09, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 1112.55, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 1093.55, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 2021.39, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 2029.73, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 2063.40, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 1453.40, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 1477.02, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 1487.17, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 1162.32, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 1160.93, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 1151.55, Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 1836.42, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 1826.52, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 1788.02, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 1258.77, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 1252.65, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 1224.61, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 991.47, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 995.79, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 995.82, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 1823.03, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 1802.90, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 1779.57, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 1241.11, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 1236.01, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 1244.32, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 985.27, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 995.38, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 1024.45, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 1851.06, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 1844.86, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 1849.61, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 1273.56, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 1272.99, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 1291.61, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 1026.55, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 1025.51, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 1054.27, Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 1107.08, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 1064.81, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 1034.38, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 754.33, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 744.04, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 752.24, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 688.62, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 680.30, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 690.19, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 1097.00, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 1055.34, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 1008.10, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 773.86, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 754.99, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 736.04, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 701.27, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 691.31, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 681.34, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 1118.68, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 1077.42, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 1027.71, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 779.70, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 752.91, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 741.97, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 712.96, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 695.12, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 690.08, Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 881.18, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 890.93, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 894.69, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 723.16, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 734.70, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 759.34, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 700.31, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 717.71, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 745.48, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 875.91, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 834.03, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 826.53, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 720.06, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 698.14, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 688.37, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 679.02, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 678.10, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 674.73, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 923.22, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 884.85, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 898.55, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 751.59, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 733.59, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 713.55, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 714.88, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 697.67, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 687.73, Parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 815.62, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 814.23, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 863.55, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 741.71, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 754.49, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 820.18, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 736.77, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 749.37, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 817.16, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 830.06, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 860.86, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 907.82, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 776.48, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 801.93, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 828.97, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 771.45, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 797.95, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 819.46, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 885.67, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 889.28, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 933.10, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 813.79, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 851.10, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 893.78, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 824.60, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 855.42, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 883.77, Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 765.06, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 736.02, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 762.27, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 674.30, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 655.82, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 679.17, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 649.82, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 638.37, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 664.01, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 762.22, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 731.96, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 731.99, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 680.96, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 660.68, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 661.98, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 661.18, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 644.20, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 644.45, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 780.30, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 742.80, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 741.62, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 703.22, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 680.25, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 685.94, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 683.08, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 666.57, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 670.38, Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 736.47, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 718.22, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 748.04, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 711.11, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 697.95, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 731.78, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 707.86, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 695.73, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 729.51, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 726.55, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 737.63, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 680.79, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 687.57, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 714.44, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 665.39, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 678.50, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 708.52, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 661.15, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 770.31, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 727.49, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 732.42, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 720.14, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 694.92, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 696.33, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 710.63, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 686.19, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 690.32, Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 760.61, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 776.09, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 805.34, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 755.54, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 771.50, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 802.44, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 755.32, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 771.39, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 802.34, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 808.01, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 806.82, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 834.84, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 802.90, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 801.71, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 824.28, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 805.69, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 800.63, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 823.61, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Mean MSE: 834.97, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Mean MSE: 876.23, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Mean MSE: 891.94, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean MSE: 823.86, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Mean MSE: 861.30, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Mean MSE: 879.28, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Mean MSE: 823.22, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Mean MSE: 857.72, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 0.9}\n",
      "Mean MSE: 876.42, Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 150, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Train the Gradient Boosting Regressor model and evaluate its performance\n",
    "print(\"Training Gradient Boosting Regressor...\")\n",
    "\n",
    "# Initialize the base Gradient Boosting model\n",
    "reg = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "print(\"Defining hyperparameter grid...\")\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # Number of boosting stages to be used.\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Shrinks the contribution of each tree.\n",
    "    'max_depth': [3, 5, 10],  # Maximum depth of the individual trees.\n",
    "    'subsample': [0.8, 0.9, 1.0],  # Fraction of samples used to fit each tree.\n",
    "    'min_samples_split': [2, 5, 10]  # Minimum samples required to split an internal node.\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "print(\"Starting Grid Search...\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=reg, \n",
    "    param_grid=param_grid, \n",
    "    cv=3, \n",
    "    scoring='neg_mean_squared_error',  # Use negative MSE as we are minimizing\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)  # Train the model with Grid Search\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "print(best_params)\n",
    "\n",
    "# Train the model using the best hyperparameters\n",
    "print(\"\\nTraining the model with the best parameters...\")\n",
    "best_reg = GradientBoostingRegressor(**best_params, random_state=42)\n",
    "best_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "print(\"\\nMaking predictions...\")\n",
    "predictions = best_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "\n",
    "# Display a summary of Grid Search results\n",
    "print(\"\\nGrid Search Results:\")\n",
    "# Loop through the Grid Search results to display each parameter combination with its mean score\n",
    "for mean_score, params in zip(grid_search.cv_results_['mean_test_score'], grid_search.cv_results_['params']):\n",
    "    print(f\"Mean MSE: {-mean_score:.2f}, Parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce48f985-2566-4f41-8d18-d4044772fbdc",
   "metadata": {},
   "source": [
    "### Step 5: Visualize in graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d553121f-fac9-4481-9ade-47a1e291a66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing feature importance...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Ridge' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisualizing feature importance...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m feature_importance \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[43mbest_reg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importances_\u001b[49m, index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m feature_importance\u001b[38;5;241m.\u001b[39msort_values()\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbarh\u001b[39m\u001b[38;5;124m'\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature Importances\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImportance\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Ridge' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "print(\"Visualizing feature importance...\")\n",
    "feature_importance = pd.Series(best_reg.feature_importances_, index=['Quantity', 'Price', 'Country'])\n",
    "feature_importance.sort_values().plot(kind='barh', title='Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plotting predictions vs actual values...\")\n",
    "plt.scatter(y_test, predictions, alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predictions vs Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0c94d3-2a30-4661-9e09-aca03ea8e3dc",
   "metadata": {},
   "source": [
    "Feature Importances:\n",
    "\n",
    "The bar chart shows the relative importance of each feature (Quantity, Price, and Country) in the Random Forest model.\n",
    "Quantity is the most important feature, contributing significantly to the predictions.\n",
    "Price also has substantial importance but less than Quantity.\n",
    "Country has minimal importance, indicating that it contributes little to the model's predictions.\n",
    "This visualization helps identify which features the model relies on most for making predictions.\n",
    "\n",
    "Predictions vs Actual Values:\n",
    "\n",
    "The scatter plot compares the predicted values (y-axis) with the actual values (x-axis) from the test data.\n",
    "The points are closely aligned along the diagonal line (y=x), indicating that the model performs well, with predictions close to the actual values.\n",
    "Some deviation can be observed for higher values, where predictions are less precise.\n",
    "This plot helps evaluate the model's accuracy and identify any systematic errors or outliers in predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58cd6d7-1a0a-4942-9d9e-639719f48cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
